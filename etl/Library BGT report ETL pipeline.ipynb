{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Extract-Burning-Glass-occupational-spotlight-reports\" data-toc-modified-id=\"Extract-Burning-Glass-occupational-spotlight-reports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><strong>Extract Burning Glass occupational spotlight reports</strong></a></span></li><li><span><a href=\"#Constants-and-generic-functions\" data-toc-modified-id=\"Constants-and-generic-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><strong>Constants and generic functions</strong></a></span></li><li><span><a href=\"#Extract-all-sectors\" data-toc-modified-id=\"Extract-all-sectors-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><strong>Extract all sectors</strong></a></span></li><li><span><a href=\"#Extraction-of-Spotlight-report-data\" data-toc-modified-id=\"Extraction-of-Spotlight-report-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><strong>Extraction of Spotlight report data</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-educational-data\" data-toc-modified-id=\"Extract-educational-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><strong>Extract educational data</strong></a></span></li><li><span><a href=\"#Extract-skill-descriptions\" data-toc-modified-id=\"Extract-skill-descriptions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span><strong>Extract skill descriptions</strong></a></span></li><li><span><a href=\"#Further-data-processing\" data-toc-modified-id=\"Further-data-processing-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span><strong>Further data processing</strong></a></span></li><li><span><a href=\"#Calculating-ranking-for-skill-composition-tables\" data-toc-modified-id=\"Calculating-ranking-for-skill-composition-tables-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span><strong>Calculating ranking for skill composition tables</strong></a></span></li></ul></li><li><span><a href=\"#Extract-Employers-data-from-reports\" data-toc-modified-id=\"Extract-Employers-data-from-reports-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><strong>Extract Employers data from reports</strong></a></span></li><li><span><a href=\"#Processing-data-for-employer-tables\" data-toc-modified-id=\"Processing-data-for-employer-tables-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><strong>Processing data for employer tables</strong></a></span></li><li><span><a href=\"#Data-extractions-for-time-series-charts\" data-toc-modified-id=\"Data-extractions-for-time-series-charts-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><strong>Data extractions for time series charts</strong></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL pipeline script\n",
    "\n",
    "## __Extract Burning Glass occupational spotlight reports__\n",
    "\n",
    "Data consists of aggregated counts of job postings for each skill and Burning Glass occupational (bgtocc) category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T08:11:16.068425Z",
     "start_time": "2019-11-05T08:10:38.401620Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inflection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-060bd0237a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertoolz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtoolz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inflection'"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "import functools\n",
    "import re\n",
    "import hashlib\n",
    "import fnmatch\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import toolz.itertoolz\n",
    "from toolz import pipe, compose\n",
    "import inflection\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T04:27:50.675939Z",
     "start_time": "2019-06-10T04:27:50.667090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started at 2019-06-10 12:27:50.668629\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "print('Job started at {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Constants and generic functions__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T08:11:16.072139Z",
     "start_time": "2019-11-05T08:10:38.394Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Pattern to extract metadata from spotlight reports\n",
    "SPOTLIGHT_REGEX = re.compile(\n",
    "    '(?P<country>US|SG).*(?P<skill_type>Baseline|Software|Specialised).*_(?P<sector>\\w+)((Mfg)?Industry|Jobs)?\\.xlsx')\n",
    "\n",
    "# generic filenames only has country and industry fields\n",
    "COUNTRY_SECTOR_REGEX = re.compile(\n",
    "    r'(?P<country>(SG|US)).*_(?P<sector>\\w+)((Mfg)?Industry|Jobs)?\\.xlsx')\n",
    "\n",
    "# NLTK TOKENIZER to split at `/` character.\n",
    "TOKENIZER = nltk.tokenize.RegexpTokenizer(r'/', gaps=True)\n",
    "\n",
    "# Concatenation of dataframes and re-indexing\n",
    "CONCATENATE_DF = functools.partial(pd.concat, ignore_index=True, sort=False)\n",
    "\n",
    "# function to locate time period of query\n",
    "\n",
    "\n",
    "def LOCATE_PERIOD(wb, filter_page='Filters'):\n",
    "    ws = wb[filter_page]\n",
    "    for cell in ws['A']:\n",
    "        m = re.match('Time Period', str(cell.value))\n",
    "        if m:\n",
    "            return ws['B'+str(cell.row)].value\n",
    "\n",
    "# Given regex return wb and meta data if the search passes the regex pattern, else return a break flag\n",
    "\n",
    "\n",
    "def CHECK_REGEX_AND_RETURN_WB(regex, filename):\n",
    "    search_result = re.search(regex, str(filename))\n",
    "    if search_result:\n",
    "        meta = search_result.groupdict()\n",
    "        wb = load_workbook(filename)\n",
    "        return meta, wb, True\n",
    "    return None, None, False\n",
    "\n",
    "# generator for data rows\n",
    "\n",
    "\n",
    "def EXTRACT_ROWS_FROM(ws_name, wb, skip_rows=1):\n",
    "    data_ws = wb[ws_name]\n",
    "    for row in itertools.islice(data_ws.iter_rows(), skip_rows, None):\n",
    "        yield row\n",
    "\n",
    "\n",
    "# Extracting sector from the sector meta data matched from the filename\n",
    "def EXTRACT_SECTOR(x):\n",
    "    search_object = re.search('(Mfg)?(Industry|Jobs)', str(x))\n",
    "    if search_object:\n",
    "        return x[:search_object.span()[0]]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# hashing function to generate unique ID from string. Required in generating links between bgtocc and skills\n",
    "# and bgtocc and their description table.\n",
    "\n",
    "\n",
    "def GENERATE_HASH(s):\n",
    "    message = str.encode(s)\n",
    "    return hashlib.md5(message).hexdigest()\n",
    "\n",
    "# extract jobs, extract year data from period, correct some sector and country name spelling\n",
    "\n",
    "\n",
    "def CLEAN_DATAFRAME(df):\n",
    "    return df.assign(year=lambda df: df.period.str.extract(r'(?P<year>20[\\d]{2})', expand=False).astype('int32')) \\\n",
    "        .assign(sector=lambda df: df.sector.map(EXTRACT_SECTOR)) \\\n",
    "        .replace({'sector': {'Acct': 'Accountacy', 'Biomed': 'Biomedical', 'HR': 'Human Resource'},\n",
    "                  'country': {'SG': 'Singapore', 'US': 'United States'}})\n",
    "\n",
    "# depluralize terms\n",
    "\n",
    "\n",
    "def DEPLURALIZE(s):\n",
    "    # processing steps:\n",
    "    # 1. Tokenize by splitting on '/'\n",
    "    # 2. Singularize tokens\n",
    "    # 3. Join singularized tokens with '/' but without the spaces.\n",
    "\n",
    "    tokens = TOKENIZER.tokenize(s)\n",
    "    s = '/'.join([inflection.singularize(token.strip())\n",
    "                  for token in tokens])\n",
    "    return s\n",
    "\n",
    "LOWER = lambda s: s.lower()\n",
    "\n",
    "def SUM_AND_RANK(df, value_var, sum_partition,\n",
    "                 rank_partition, order_by, ascending=False):\n",
    "    \n",
    "    return df.groupby(sum_partition, as_index=False)[value_var] \\\n",
    "        .sum() \\\n",
    "        .assign(\n",
    "            rank=lambda df: df.groupby(rank_partition)[order_by]\n",
    "        .transform(lambda s: s.rank(method='first', ascending=ascending)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T08:11:16.076928Z",
     "start_time": "2019-11-05T08:10:38.397Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "root = pathlib.Path('.')\n",
    "print('Creating table_file folder')\n",
    "try:\n",
    "    os.makedirs('table_files')\n",
    "except:\n",
    "    print('Folder already present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Extract all sectors__\n",
    "\n",
    "Sector names are extracted from filenames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T04:27:50.753583Z",
     "start_time": "2019-06-10T04:27:50.719238Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Extracting sector names\n",
      "================================================================================\n",
      "Reading filenames...\n",
      "Unique sectors extracted\n",
      "File saved to sector_table.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Run through the entire collection of reports extracting sector names from the filenames\n",
    "# using regex pattern matching .\n",
    "\n",
    "SECTOR_TABLE = 'sector_table.csv'\n",
    "\n",
    "print('\\n', '='*80, 'Extracting sector names',\n",
    "      '='*80, 'Reading filenames...', sep='\\n')\n",
    "sectors = []\n",
    "for filename in root.rglob('*.xlsx'):\n",
    "    sector = re.search(COUNTRY_SECTOR_REGEX, str(filename)).group('sector')\n",
    "    sector = EXTRACT_SECTOR(sector)\n",
    "    sectors.append(sector)\n",
    "unique_sectors = set(sectors)\n",
    "\n",
    "# Save data to flat file .\n",
    "pd.DataFrame(unique_sectors, columns=['sector']) \\\n",
    "    .replace({'HR': 'Human Resource', 'Biomed': 'Biomedical', 'Acct': 'Accountacy'}) \\\n",
    "    .to_csv(root / 'table_files' / SECTOR_TABLE, index=True, index_label='id')\n",
    "\n",
    "print('Unique sectors extracted',\n",
    "      f'File saved to {SECTOR_TABLE}',  'Done', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Extraction of Spotlight report data__ \n",
    "\n",
    "Data from the spotlight reports consist of occupational categories with their required skills. Counts of how many job advertistments that contain these terms are provided. \n",
    "\n",
    "Data is organised in the form of \n",
    "\n",
    "\n",
    "| BGTOCC    | job_postings |\n",
    "|-----| -----| \n",
    "| $jobtitle_1$| $n_1$ |\n",
    "| $skill_{11}$ | $m_{11}$ | \n",
    "| $skill_{12}$ | $m_{12}$ | \n",
    "|  ... | ... |\n",
    "| $jobtitle_2$ | $n_2$| \n",
    "| $skill_{21}$ | $m_{21}$|\n",
    "\n",
    "### __Extract educational data__\n",
    "\n",
    "Data for educational requirements are located in Spotlight reports on the `Report4_Data` worksheet. \n",
    "\n",
    "It's original form is unstacked.  \n",
    "\n",
    "### __Extract skill descriptions__\n",
    "\n",
    "Skill descriptions can be found in `Report3_Data` in US sheets. In order to ensure consistency with and to link descriptions to skill terms used in Singapore, the strings are depluralised, lowercased and then hashed. This hash is our linkage to the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:30:29.195751Z",
     "start_time": "2019-06-11T06:29:35.653828Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Extracting data from Spotlight reports\n",
      "================================================================================\n",
      "Read 264 files  \r"
     ]
    }
   ],
   "source": [
    "file_list = (root / 'data').rglob('*Spotlight*BGTOCC*.xlsx')\n",
    "\n",
    "print('\\n', '='*80, 'Extracting data from Spotlight reports', '='*80, sep='\\n')\n",
    "\n",
    "# main loop\n",
    "bgtocc_data = []\n",
    "education_data = []\n",
    "skill_description_data = []\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    meta, wb, status = CHECK_REGEX_AND_RETURN_WB(SPOTLIGHT_REGEX, filename)\n",
    "    if status:\n",
    "        # find the period of the query\n",
    "        period = LOCATE_PERIOD(wb)\n",
    "\n",
    "        # extract data from ws\n",
    "\n",
    "        for row in EXTRACT_ROWS_FROM('Report1_Data', wb):\n",
    "            bgtocc_hashed_term = pipe(row[0].value, LOWER, DEPLURALIZE, GENERATE_HASH)\n",
    "            bgtocc_data.append(dict(**{\n",
    "                'bgtocc': row[0].value,\n",
    "                'job_postings': row[1].value,\n",
    "                'type': 'BGTOCC' if row[0].alignment.indent == 0 else 'skill',\n",
    "                'period': period,\n",
    "                'hashed_term': bgtocc_hashed_term\n",
    "            }, **meta))\n",
    "\n",
    "        # extract educational data from SG reports only\n",
    "        if meta['country'] == 'SG':\n",
    "            sheet_number = 4\n",
    "            while sheet_number<=8:\n",
    "                use_sheet = f'Report{sheet_number}_Data'\n",
    "                try: \n",
    "                    df = pd.read_excel(filename, use_sheet, usecols='A:E') \\\n",
    "                        .dropna() \\\n",
    "                        .pipe(pd.melt, id_vars='Experience', var_name='Education',\n",
    "                              value_name='num_of_jobs') \\\n",
    "                        .assign(period=period,\n",
    "                                num_of_jobs=lambda df: df.num_of_jobs.astype('int32'),\n",
    "                                **meta) \\\n",
    "                        .drop('skill_type', axis=1) \\\n",
    "                        .drop_duplicates()\n",
    "                    break\n",
    "                except:\n",
    "                    sheet_number += 1\n",
    "                    continue\n",
    "            education_data.append(df)\n",
    "\n",
    "        # Extraction of skill descriptions\n",
    "        if meta['country'] == 'US':\n",
    "            df = pd.read_excel(filename, 'Report3_Data', usecols='A:B') \\\n",
    "                .dropna() \\\n",
    "                .rename({'Skills': 'skill_name'}, axis=1) \\\n",
    "                .assign(\n",
    "                skill_name_hash=lambda df: df.skill_name.str.lower().map(compose(GENERATE_HASH, DEPLURALIZE)))\n",
    "            skill_description_data.append(df)\n",
    "\n",
    "        print(f'Read {i+1} files  ', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:30:33.351642Z",
     "start_time": "2019-06-11T06:30:29.200467Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data ...\n",
      "Saving data to bgtocc_table.csv, skill_table.csv, education_table.csv and skill_description_table.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "EDUCATION_TABLE = 'education_table.csv'\n",
    "BGTOCC_TABLE = 'bgtocc_table.csv'\n",
    "SKILL_TABLE = 'skill_table.csv'\n",
    "SKILL_DESCRIPTION_TABLE = 'skill_description_table.csv'\n",
    "\n",
    "print('\\nProcessing data ...')\n",
    "\n",
    "\n",
    "# Processing for bgtocc and skill.\n",
    "df = pd.DataFrame(bgtocc_data)\n",
    "\n",
    "\n",
    "# generate hashes for each bgtocc term. We use this as a link between skills and bgtocc.\n",
    "# Use ffill to pad out NaN values. This ensures correct parentage between skill and bgtocc\n",
    "\n",
    "df = df.assign(index=df.apply(\n",
    "    lambda s: GENERATE_HASH(s.bgtocc+s.period+s.country +\n",
    "                            s.sector+s.skill_type) if s.type == 'BGTOCC' else np.nan,\n",
    "    axis=1)) \\\n",
    "    .fillna(method='ffill')\n",
    "\n",
    "# extract jobs, extract year data from period, correct some sector and country name spelling\n",
    "bgtocc_df = df[df.type == 'BGTOCC'] \\\n",
    "    .drop(['skill_type', 'type'], axis=1) \\\n",
    "    .rename({'hashed_term': 'bgtocc_name_hash'}, axis=1) \\\n",
    "    .pipe(CLEAN_DATAFRAME)\n",
    "\n",
    "# extract skills. bgtocc_index ensures correct linkage to bgtocc_df table.\n",
    "# Include column `hashed_term` to ensure correct linkage to description table.\n",
    "skill_df = df.loc[df.type == 'skill', ['index', 'bgtocc', 'skill_type',\n",
    "                                       'hashed_term', 'job_postings']] \\\n",
    "    .rename({'index': 'bgtocc_index',\n",
    "             'bgtocc': 'skill_name',\n",
    "             'hashed_term': 'skill_name_hash'}, axis=1)\n",
    "\n",
    "bgtocc_df.to_csv(str(root / 'table_files' / BGTOCC_TABLE), index=False)\n",
    "skill_df.to_csv(str(root / 'table_files' / SKILL_TABLE), index_label='id')\n",
    "\n",
    "# Saving data for descriptions. As we intend that the descriptions table be the\n",
    "# single source of truth, we push `skill_df` to the end of the collection,\n",
    "# assign it an extra field `Decriptions` to align with the previous collections ,\n",
    "# and drop duplicated hashes keeping the first occurences.\n",
    "\n",
    "extra_skill_descriptions_df = skill_df.loc[:, ['skill_name', 'skill_name_hash']] \\\n",
    "    .assign(Description='')\n",
    "\n",
    "skill_description_data.append(extra_skill_descriptions_df)\n",
    "\n",
    "pipe(skill_description_data, CONCATENATE_DF) \\\n",
    "    .drop_duplicates('skill_name_hash', keep='first') \\\n",
    "    .to_csv(\n",
    "    str(root / 'table_files' / SKILL_DESCRIPTION_TABLE), index_label='id', sep=';')\n",
    "\n",
    "# Saving data for education\n",
    "pipe(education_data, CONCATENATE_DF, CLEAN_DATAFRAME) \\\n",
    "    .drop_duplicates()\\\n",
    "    .groupby(['Experience', 'Education', 'country', 'sector', 'year'], as_index=False) \\\n",
    "    .agg({'num_of_jobs': 'sum'}) \\\n",
    "    .assign(education_rank=lambda df: df.Education.str.extract(r'B0(\\d)', expand=False)) \\\n",
    "    .assign(experience_rank=lambda df: df.Experience.str.extract(r'(\\d)', expand=False)) \\\n",
    "    .fillna(-1) \\\n",
    "    .to_csv(str(root / 'table_files' / EDUCATION_TABLE), index_label='id')\n",
    "\n",
    "# dump dataframe into flat file\n",
    "print(\n",
    "    f'Saving data to {BGTOCC_TABLE}, {SKILL_TABLE}, {EDUCATION_TABLE} and {SKILL_DESCRIPTION_TABLE}')\n",
    "\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Further data processing__\n",
    "\n",
    "Creating tables for job postings totals and skill rankings in terms of totals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:30:39.754688Z",
     "start_time": "2019-06-11T06:30:39.333560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging dataframes...\n",
      "Calculating totals and rankings...\n",
      "Data frame saved to skill_rank_table.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "SKILL_RANK_TABLE = 'skill_rank_table.csv'\n",
    "\n",
    "print('Merging dataframes...')\n",
    "merge_df = bgtocc_df.merge(skill_df, left_on='index',\n",
    "                           right_on='bgtocc_index', suffixes=('_bgtocc', '_skill'))\n",
    "\n",
    "print('Calculating totals and rankings...')\n",
    "skill_rank_by_year = merge_df.pipe(SUM_AND_RANK, 'job_postings_skill',\n",
    "                                   ['country', 'sector', 'year', 'skill_name_hash'],\n",
    "                                   ['country', 'sector', 'year'], 'job_postings_skill')\n",
    "\n",
    "skill_rank_overall = merge_df.pipe(SUM_AND_RANK, 'job_postings_skill',\n",
    "                                   ['country', 'sector', 'skill_name_hash'],\n",
    "                                   ['country', 'sector'], 'job_postings_skill')\\\n",
    "    .assign(year=9999)\n",
    "\n",
    "print(f'Data frame saved to {SKILL_RANK_TABLE}')\n",
    "skill_rank_df = pd.concat([skill_rank_by_year, skill_rank_overall], sort=True, ignore_index=True)\\\n",
    "    .to_csv(root / 'table_files' / SKILL_RANK_TABLE, index_label='id')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Calculating ranking for skill composition tables__ \n",
    "\n",
    "To aid ordering of results, we calculate the three year totals for all skills in a Burning Glass occupational category and rank them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:34:31.086663Z",
     "start_time": "2019-06-11T06:34:28.325917Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating skill composition dataframe\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "SKILL_COMPOSITION_TABLE = 'skill_composition_table.csv'\n",
    "\n",
    "print('\\nCreating skill composition dataframe')\n",
    "skill_composition_df = merge_df.pipe(SUM_AND_RANK, 'job_postings_skill',\n",
    "                                     ['country', 'bgtocc_name_hash', 'sector', 'skill_type',\n",
    "                                      'skill_name_hash'],\n",
    "                                     ['country', 'bgtocc_name_hash',\n",
    "                                         'sector', 'skill_type'],\n",
    "                                     'job_postings_skill')\\\n",
    "    .loc[:, ['country', 'bgtocc_name_hash', 'sector', 'skill_type', 'skill_name_hash', \n",
    "             'rank']]\\\n",
    "    .merge(merge_df, on=['country', 'bgtocc_name_hash',\n",
    "                         'sector', 'skill_type', 'skill_name_hash']) \\\n",
    "    .groupby(['country', 'bgtocc_name_hash', 'sector', 'skill_type',\n",
    "              'skill_name_hash', 'bgtocc', 'year', 'skill_name'], as_index=False) \\\n",
    "    .agg({'rank': 'mean',\n",
    "          'job_postings_skill': 'sum'}) \\\n",
    "    .assign(num_years=lambda df: df.groupby(['sector', 'country', 'bgtocc_name_hash'])\n",
    "            ['year']\n",
    "            .transform(lambda s: s.nunique())) \\\n",
    "    .assign(num_countries=lambda df: df.groupby(['sector', 'bgtocc_name_hash'])\n",
    "            ['country']\n",
    "            .transform(lambda s: s.nunique()))\n",
    "\n",
    "print(f'Saving dataframe to {SKILL_COMPOSITION_TABLE}')\n",
    "skill_composition_df.to_csv(root / 'table_files' / SKILL_COMPOSITION_TABLE, index_label='id')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Extract Employers data from reports__\n",
    "\n",
    "Data consists of a two columns one with employer names and number of job postings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T04:29:02.523226Z",
     "start_time": "2019-06-10T04:28:55.332417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Extracting data from Employers reports\n",
      "================================================================================\n",
      "Read 116 files...\n",
      "Data saved to employer_table.csv\n"
     ]
    }
   ],
   "source": [
    "print('\\n', '='*80, 'Extracting data from Employers reports', '='*80, sep='\\n')\n",
    "\n",
    "files_list = (root / 'data').rglob('*Employers*.xlsx')\n",
    "\n",
    "EMPLOYER_TABLE = 'employer_table.csv'\n",
    "EMPLOYER_RANK_TABLE = 'employer_ranked_table.csv'\n",
    "\n",
    "employer_data = []\n",
    "for i, filename in enumerate(files_list):\n",
    "    meta, wb, status = CHECK_REGEX_AND_RETURN_WB(\n",
    "        COUNTRY_SECTOR_REGEX, filename)\n",
    "\n",
    "    if status:\n",
    "        period = LOCATE_PERIOD(wb)\n",
    "\n",
    "        # extract data from ws\n",
    "        df = pd.read_excel(filename, 'Data', usecols='A:B') \\\n",
    "            .dropna() \\\n",
    "            .assign(period=period, **meta)\n",
    "\n",
    "        employer_data.append(df)\n",
    "        print(f'Read {i+1} files...',  end='\\r')\n",
    "\n",
    "# and pipe the df through to the cleaning pipeline\n",
    "employer_df = pipe(employer_data, CONCATENATE_DF, CLEAN_DATAFRAME) \\\n",
    "    .rename({'Job Postings': 'job_postings'}, axis=1)\n",
    "\n",
    "employer_df.to_csv(root / 'table_files' / EMPLOYER_TABLE, index_label='id')\n",
    "print(f'\\nData saved to {EMPLOYER_TABLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Processing data for employer tables__ \n",
    "\n",
    "We filter down to the years from 2016 and rank employers by total job postings over this time period. This is to aid in the ordering of the axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T04:29:02.856839Z",
     "start_time": "2019-06-10T04:29:02.527145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting employer ranks\n",
      "Ranking data saved to employer_ranked_table.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Ranking employers with country and sector in terms of total number of jobs postings for the last \n",
    "# three years \n",
    "print('Getting employer ranks')\n",
    "filtered_employer_df = employer_df.query('year >= 2016')\n",
    "pd.merge(filtered_employer_df.groupby(['country', 'sector', 'Employer', 'year'], as_index=False)\n",
    "         .agg({'job_postings': 'sum'}),\n",
    "         filtered_employer_df.pipe(SUM_AND_RANK, 'job_postings',\n",
    "                          ['country', 'sector', 'Employer'],\n",
    "                          ['country', 'sector'],\n",
    "                          'job_postings'),\n",
    "         on=['country', 'sector', 'Employer'], \n",
    "        suffixes=('', '_total')) \\\n",
    "    .to_csv(str(root / 'table_files' / EMPLOYER_RANK_TABLE), index=True, index_label='id')\n",
    "\n",
    "print(f'Ranking data saved to {EMPLOYER_RANK_TABLE}')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data extractions for time series charts__\n",
    "\n",
    "Data is taken from excel files named TopBGTOCCs. It contains 3 fields (4 for US) namely BGTOCC code, BGTOCC and job_postings (with additional Description for US). \n",
    "\n",
    "We require descriptions for both Singapore and US. Descriptions will be normalized to another table with linkage provided by a key formed by hashing the BGTOCC title. \n",
    "\n",
    "To facilitate that, the terms used needs to be tokenised and depluralised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:11:25.014019Z",
     "start_time": "2019-06-17T07:11:18.787475Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Extracting data from Top BGTOCC reports\n",
      "================================================================================\n",
      "Read 148 files..\r"
     ]
    }
   ],
   "source": [
    "file_list = (root / 'data').rglob('*TopBGTOCCs*.xlsx')\n",
    "\n",
    "print('\\n', '='*80, 'Extracting data from Top BGTOCC reports', '='*80, sep='\\n')\n",
    "data = []\n",
    "for i, filename in enumerate(file_list):\n",
    "    meta, wb, status = CHECK_REGEX_AND_RETURN_WB(\n",
    "        COUNTRY_SECTOR_REGEX, filename)\n",
    "    if status:\n",
    "        period = LOCATE_PERIOD(wb)\n",
    "\n",
    "        for row in EXTRACT_ROWS_FROM('Data', wb):\n",
    "            rv = row[1].value\n",
    "            if rv:\n",
    "\n",
    "                # exceptions:\n",
    "                # 1. Spelling error for Registrar\n",
    "                rgx = re.search(r'(.*)Registar', rv)\n",
    "                if rgx:\n",
    "                    rv = rgx.group(0)+'Registrar'\n",
    "\n",
    "                # print(rv)\n",
    "                data.append(dict(**{\n",
    "                    'bgtocc': rv,\n",
    "                    'bgtocc_hash': pipe(rv, LOWER, DEPLURALIZE, GENERATE_HASH),\n",
    "                    'period': period,\n",
    "                    'description': row[2].value if meta['country'] == 'US' else '',\n",
    "                    'job_postings': row[3].value if meta['country'] == 'US' else row[2].value,\n",
    "\n",
    "                }, **meta))\n",
    "        print(f'Read {i+1} files..', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:11:31.517709Z",
     "start_time": "2019-06-17T07:11:25.017393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "Data saved to file top_bgtoccs_table.csv and bgtocc_description_table.csv\n",
      "Computing yearly rank and percentages\n",
      "Compute overall rank\n",
      "Saving dataframe to file  top_bgtoccs_rank_and_perc.csv\n"
     ]
    }
   ],
   "source": [
    "TOP_BGTOCCS_TABLE = 'top_bgtoccs_table.csv'\n",
    "BGTOCC_DESCRIPTION_TABLE = 'bgtocc_description_table.csv'\n",
    "TOP_BGTOCCS_RANK_AND_PERCENTAGE_TABLE = 'top_bgtoccs_rank_and_perc.csv'\n",
    "\n",
    "print('Processing data ...')\n",
    "\n",
    "\n",
    "# Construct data frame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# normalize data by sorting by description and dropping duplicate hashes\n",
    "description_df = df.loc[:, ['bgtocc', 'bgtocc_hash', 'description']] \\\n",
    "    .sort_values(by='description', ascending=False) \\\n",
    "    .drop_duplicates('bgtocc_hash')\n",
    "\n",
    "# drop description field and clean dataframe\n",
    "top_bgtoccs_df = pipe(df.drop('description', axis=1), CLEAN_DATAFRAME)\n",
    "\n",
    "# save data to file\n",
    "top_bgtoccs_df.to_csv(\n",
    "    str(root / 'table_files' / TOP_BGTOCCS_TABLE), index_label='id')\n",
    "description_df.to_csv(\n",
    "    str(root / 'table_files' / BGTOCC_DESCRIPTION_TABLE), index_label='id', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print('Data saved to file {} and {}'.format(\n",
    "    TOP_BGTOCCS_TABLE, BGTOCC_DESCRIPTION_TABLE))\n",
    "\n",
    "print('Computing yearly rank and percentages')\n",
    "top_bgtoccs_year_rank_df = top_bgtoccs_df.pipe(SUM_AND_RANK, 'job_postings',\n",
    "                                               ['bgtocc_hash', 'country',\n",
    "                                                   'sector', 'year'],\n",
    "                                               ['country', 'sector', 'year'],\n",
    "                                               'job_postings') \\\n",
    "    .assign(job_postings_total=lambda df: df.groupby(['country', 'sector', 'year'])\n",
    "            ['job_postings']\n",
    "            .transform('sum')) \\\n",
    "    .assign(percentage=lambda df: 100 * df.job_postings.div(df.job_postings_total))\n",
    "\n",
    "print('Compute overall rank')\n",
    "top_bgtoccs_rank_overall_df = top_bgtoccs_df.pipe(SUM_AND_RANK, 'job_postings',\n",
    "                                                  ['bgtocc_hash',\n",
    "                                                      'country', 'sector'],\n",
    "                                                  ['country', 'sector'],\n",
    "                                                  'job_postings') \\\n",
    "    .assign(year=9999)\n",
    "\n",
    "print(f'Saving dataframe to file  {TOP_BGTOCCS_RANK_AND_PERCENTAGE_TABLE}')\n",
    "pd.concat([top_bgtoccs_rank_overall_df, top_bgtoccs_year_rank_df], sort=True, ignore_index=True) \\\n",
    "    .fillna(0) \\\n",
    "    .merge(top_bgtoccs_df[['bgtocc_hash', 'bgtocc']].drop_duplicates(), on='bgtocc_hash') \\\n",
    "    .assign(num_countries=lambda df: df.groupby(['bgtocc_hash', 'sector'])['country']\n",
    "            .transform(lambda s: s.nunique())) \\\n",
    "    .assign(num_years=lambda df: df.groupby(['bgtocc_hash', 'sector', 'country'])['year']\n",
    "            .transform(lambda s: s[(s < 9999) & (s >= 2014)].nunique())) \\\n",
    "    .assign(num_years=lambda df: df.groupby(['bgtocc_hash', 'sector'])[\n",
    "        'num_years']\n",
    "    .transform(lambda s: s.min())) \\\n",
    "    .to_csv(root / 'table_files' / TOP_BGTOCCS_RANK_AND_PERCENTAGE_TABLE, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T04:29:08.201761Z",
     "start_time": "2019-06-10T04:29:08.195804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished at 2019-06-10 12:29:08.197948 Job took 77.52929878234863 to complete\n"
     ]
    }
   ],
   "source": [
    "time_taken = time.time()-time0\n",
    "\n",
    "print(f'Job finished at {datetime.datetime.now()}',\n",
    "      f'Job took {time_taken} to complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End script"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
